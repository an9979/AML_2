{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic_df = pd.read_csv('test.csv')\n",
    "# Drop irrelevant columns\n",
    "titanic_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)\n",
    "titanic_df['Cabin'].fillna('Unknown', inplace=True)\n",
    "titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical variables to numerical\n",
    "titanic_df['Sex'] = titanic_df['Sex'].map({'male': 0, 'female': 1})\n",
    "titanic_df['Cabin'] = titanic_df['Cabin'].str[0]\n",
    "titanic_df['Cabin'] = titanic_df['Cabin'].map(\n",
    "    {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'T': 7, 'Unknown': 8})\n",
    "titanic_df['Embarked'] = titanic_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "\n",
    "# Split the data into features and target\n",
    "X = titanic_df.drop('Survived', axis=1)\n",
    "y = titanic_df['Survived']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the decision tree model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the decision tree model with the best hyperparameters\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=5, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "\n",
    "# Train the decision tree model on the training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# Export the decision tree to DOT format\n",
    "dot_data = export_graphviz(dt, out_file=None,\n",
    "                           feature_names=X_train.columns,\n",
    "                           class_names=['Not survived', 'Survived'],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "\n",
    "# Visualize the decision tree with Graphviz\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('titanic_decision_tree')  # Save the decision tree as a PDF file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the decision tree model with the best hyperparameters\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=5, min_samples_leaf=1, min_samples_split=2, max_features='sqrt')\n",
    "\n",
    "# Train the decision tree model on the training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Determine the optimal alpha value for cost complexity pruning\n",
    "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Train decision trees with different alpha values and evaluate their performance\n",
    "dts = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    dt = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1,\n",
    "                                min_samples_split=2, max_features='sqrt', ccp_alpha=ccp_alpha)\n",
    "    dt.fit(X_train, y_train)\n",
    "    dts.append(dt)\n",
    "\n",
    "# Evaluate the performance of the decision trees with different alpha values on the validation data\n",
    "train_scores = [dt.score(X_train, y_train) for dt in dts]\n",
    "test_scores = [dt.score(X_test, y_test) for dt in dts]\n",
    "\n",
    "# Find the optimal alpha value that maximizes the test score\n",
    "optimal_alpha = ccp_alphas[test_scores.index(max(test_scores))]\n",
    "\n",
    "# Train the decision tree with the optimal alpha value on the full training data\n",
    "dt_pruned = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1,\n",
    "                                   min_samples_split=2, max_features='sqrt', ccp_alpha=optimal_alpha)\n",
    "dt_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Report the performance metrics of the pruned decision tree on the test data\n",
    "y_pred_pruned = dt_pruned.predict(X_test)\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "precision_pruned = precision_score(y_test, y_pred_pruned)\n",
    "recall_pruned = recall_score(y_test, y_pred_pruned)\n",
    "f1_pruned = f1_score(y_test, y_pred_pruned)\n",
    "print(\"Pruned Decision Tree:\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_pruned))\n",
    "print(\"Precision: {:.2f}\".format(precision_pruned))\n",
    "print(\"Recall: {:.2f}\".format(recall_pruned))\n",
    "print(\"F1-score: {:.2f}\".format(f1_pruned))\n",
    "\n",
    "# Visualize the pruned decision tree\n",
    "dot_data_pruned = export_graphviz(dt_pruned, out_file=None,\n",
    "                                  feature_names=X_train.columns,\n",
    "                                  class_names=['Not survived', 'Survived'],\n",
    "                                  filled=True, rounded=True,\n",
    "                                  special_characters=True)\n",
    "graph_pruned = graphviz.Source(dot_data_pruned)\n",
    "graph_pruned.render('pruned_titanic_decision_tree')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
